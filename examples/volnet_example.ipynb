{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise the SympNet\n",
    "\n",
    "We will use symplectic neural networks with quadratic ridge polynomials (which is the best method for quadratic Hamiltonians) by setting the optional arguments `max_degree=2` and `method='P'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent path to sys.path to import utils\n",
    "from strupnet import VolNet, SympNet\n",
    "import torch\n",
    "\n",
    "DIM = 3  # dimension of the ODE\n",
    "\n",
    "sympnet_kwargs = dict(\n",
    "    layers=6,\n",
    "    width=8,\n",
    "    method=\"R\",\n",
    ")\n",
    "\n",
    "volnet = VolNet(dim=DIM, **sympnet_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data \n",
    "\n",
    "We will generate data of the form $ \\{x(ih)\\}_{i=0}^{n+1}$, where $\\psi_h(x(0))=x(h)$ is a volume preserving map with unit determinant. The data is arranged in the form $ x_0 = \\{x(ih)\\}_{i=0}^{n} $, $ x_1 = \\{x((i+1)h)\\}_{i=0}^{n} $ and same for $ t $. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "def div_free_ode(x):\n",
    "    \"\"\"Create div-free ODE where d/dx_i(f_i)=0\"\"\"\n",
    "    return (\n",
    "        torch.sin(x[..., 1] * x[..., 2]),\n",
    "        torch.cos(-x[..., 0] * x[..., 2]),\n",
    "        torch.cos(-x[..., 0] * x[..., 1]),\n",
    "    )\n",
    "\n",
    "\n",
    "def volume_preserving_map(x_in, h, f=div_free_ode):\n",
    "    \"\"\"Create a volume preserving map using a splitting method\"\"\"\n",
    "    x = x_in.clone()\n",
    "    f0, _, _ = f(x)\n",
    "    x[:, 0] = x[:, 0] + h * f0\n",
    "    _, f1, _ = f(x)\n",
    "    x[:, 1] = x[:, 1] + h * f1\n",
    "    _, _, f2 = f(x)\n",
    "    x[:, 2] = x[:, 2] + h * f2\n",
    "    return x\n",
    "\n",
    "\n",
    "def volume_preserving_data(timestep=0.05, n_points=100):\n",
    "    x0 = torch.rand(n_points, DIM) * 2 - 1\n",
    "    x1 = volume_preserving_map(x0, h=timestep)\n",
    "    t0 = torch.zeros(n_points, 1)\n",
    "    t1 = timestep * torch.ones(n_points, 1)\n",
    "    return x0, x1, t0, t1\n",
    "\n",
    "TIMESTEP = 0.1\n",
    "x0, x1, t0, t1 = volume_preserving_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the `VolNet` like any PyTorch model \n",
    "All the models in `strupnet` inherit from `torch.nn.Module` and can be trained as such. The loss function can be defined as follows. Letting $\\Phi_h^{\\theta}(x)$ denote the network, where $\\theta$ denotes its set of trainable parameters, then we want to find $\\theta$ that minimises \n",
    "\n",
    "$\\qquad loss=\\sum_{i=0}^{n}\\|\\Phi_h^{\\theta}(x(ih))-x\\left((i+1)h\\right)\\|^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.001589333537069788\n",
      "Epoch:  1000 Loss:  4.596445511723816e-06\n",
      "Epoch:  2000 Loss:  2.447653168846114e-06\n",
      "Epoch:  3000 Loss:  1.38057978441213e-06\n",
      "Epoch:  4000 Loss:  8.227833700142502e-07\n",
      "Epoch:  5000 Loss:  7.382183144652395e-07\n",
      "Epoch:  6000 Loss:  6.936953094251066e-07\n",
      "Epoch:  7000 Loss:  6.142210215917696e-07\n",
      "Epoch:  8000 Loss:  5.638876662020627e-07\n",
      "Epoch:  9000 Loss:  7.436061074065027e-07\n",
      "Final loss value:  5.043130712668192e-07\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(volnet.parameters(), lr=0.01)\n",
    "mse = torch.nn.MSELoss()\n",
    "for epoch in range(10000):\n",
    "    optimizer.zero_grad()    \n",
    "    x1_pred = volnet(x=x0, dt=t1 - t0)\n",
    "    loss = mse(x1, x1_pred)\n",
    "    print(\"Epoch: \", epoch, \"Loss: \", loss.item()) if epoch % 1000 == 0 else None\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Final loss value: \", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the trained model on a test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss value on test data:  1.7377077673535785e-06\n"
     ]
    }
   ],
   "source": [
    "x0, x1, t0, t1 = volume_preserving_data()\n",
    "x1_pred = volnet(x=x0, dt=t1 - t0)\n",
    "loss = mse(x1, x1_pred)\n",
    "print(\"Final loss value on test data: \", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that the volnet has unit Jacobian determinant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The determinant of the volnets evaluated on the test sets have mean absolute error:\n",
      "\t 2.2870594307278225e-16\n"
     ]
    }
   ],
   "source": [
    "# compute jacobians of volnet at the points x0 in test set\n",
    "jacobians = torch.autograd.functional.jacobian(lambda x: volnet(x, 0.1), x0).sum(2).reshape(-1, DIM, DIM)\n",
    "determinants = torch.det(jacobians)\n",
    "mae = torch.mean(torch.abs(determinants - 1))\n",
    "print(\"The determinant of the volnet evaluated on the test set has mean absolute error:\\n\\t\", mae.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
